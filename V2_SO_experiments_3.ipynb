{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1bgXK9-Hsbc_U5oFFImTMSGh3QPyY4Yyv",
      "authorship_tag": "ABX9TyPp1cAbUKGAoxBpkLT4feW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefanOglesby/chatbot/blob/main/V2_SO_experiments_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1+1\n",
        "#ERROR IN ROW 457 must be corrected\n",
        "#IMPORT python-docx REPLACING spire.\n",
        "# use python-docx: The changes affect line 311 and lines 341 to 411 of the code.\n",
        "################################################################################\n",
        "###\n",
        "###  RAG PROJECT FOR G-I-M: Knowledge management for \"qual\" discussion guides and\n",
        "###  \"quant\" online survey questionnaires.\n",
        "#### reading, parsing and embedding and querying word and excel documents\n",
        "################################################################################\n",
        "#\n",
        "# The script has 4 SECTIONS:\n",
        "# Sections I, II and III load and transform and index data for the knowledge base.\n",
        "# Section IV is the backend for an interactive tool allowing to retrieve knowledge.\n",
        "#\n",
        "# SECTION I: reading the .docx and .xlsx files, creating a dictionary w/ metadata\n",
        "# for each file. For \"qual\" documents, each paragraph is extracted as a string in\n",
        "# a list [\"text 1\"]. For \"quant\" documents, each row of a table in the word document\n",
        "# or the excel document is extracted as a list of stings [*cell 1\", \"cell 2\", \"cell 3\"]\n",
        "#\n",
        "#  dict_1 = {\"ID\": ID, \"method\": method, \"language\": language, \"country\": country,\n",
        "#        \"file_name\": file_name, \"file_path\": file_path, \"payload\":\n",
        "#            {\"format\": \"x\", \"content\": [[\"cell 1\", \"cell 2\", \"cell 3\"]\n",
        "#                                           [\"cell 1\", \"cell 2\", \"cell 3\"]]},\n",
        "#            {\"format:\" \"p\", \"content\": [[\"text 1\"], [\"text 2\"], [\"text 3\"]]}}\n",
        "#\n",
        "#  formats: p = paragraph in .docx, t = table in .docx, x = .xlsx\n",
        "#\n",
        "# SECTION II: extracting the relevant text strings for embedding / indexing.\n",
        "# For \"qual\" document, each paragraph is appended to the list of relevant strings.\n",
        "# For \"quant\" documents, the questionstrings within the rows are identified\n",
        "# and only the question-strings are appended to the list of relevant strings.\n",
        "# Meta-Data for each string selected is added: Document ID, and the location in the\n",
        "# documents dictionary.\n",
        "#\n",
        "# q_list is the list of relevant strings:  [\"cell 5\", \"text 10\", \"cell 2\"]\n",
        "#\n",
        "# SECTION III: merging of the strings selected for embedding with metadata that is\n",
        "# required for filtering documents in the index database during queries, e.g. for method\n",
        "# (\"qual\" or \"quant\"), country or language.\n",
        "# Embedding the strings using Azure openAI API embedding model, and merging the indexes\n",
        "# with the document dictionaries and the metadata into a dataframe.\n",
        "#\n",
        "# df_merged is te resulting combination of index database and metadata all in one dataframe for\n",
        "# easy retrieval in the query section.\n",
        "# For production, the indexes will be stored in an index database,  e.g. qdrant, together\n",
        "# with payload = metadata that is required for filtering the index database during queries\n",
        "# , plus an  identifier (ID) for the original document, that allows to retrieve the original\n",
        "# content related to the results of the query plus additional metadata, such as the link\n",
        "# to the related .docx or .xlsx document.\n",
        "#\n",
        "# SECTIONn IV: submitting queries, getting top 3 matches using cosine similarity, extracting\n",
        "# and printing top 3 text chunks with metadata (method, file name, file link)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###### I N I T I A L I S A T I O N #############################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "!pip3 install llama_index\n",
        "!pip3 install llama_index.llms.azure_openai\n",
        "!pip3 install llama_index.embeddings.azure_openai\n",
        "\n",
        "!pip3 install sklearn.metrics.pairwise\n",
        "\n",
        "# We need to install the key for the paid, full version of spire\n",
        "#############   replaced by pyhton-docx !pip install spire.doc\n",
        "\n",
        "!pip3 install python-docx\n",
        "# !pip3 install docx2python\n",
        "\n",
        "!pip3 install docx2txt\n",
        "\n",
        "# !pip3 install mistralai\n",
        "\n",
        "#### for semantic splitting into chunks\n",
        "\n",
        "# !pip3 install llama_index.node_parser\n",
        "# !pip3 install llama_index.text_splitter\n",
        "\n",
        "\n",
        "1+1\n",
        "# from docx2python import docx2python\n",
        "## original prototype\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "## load Document in order to manually create llama index document\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.readers.file import DocxReader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# # import streamlit as st\n",
        "# import logging\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "## additional for importing documents\n",
        "import docx2txt\n",
        "\n",
        "## alternative to spire.doc\n",
        "import docx\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "# from spire.doc import *\n",
        "# from spire.doc.common import *\n",
        "\n",
        "# from spire.doc.common import License as docLicense\n",
        "\n",
        "## Apply license for Spire.Doc\n",
        "# docLicense.SetLicenseKey(\n",
        "# \"3QEAC+/ahTUBEe5UDEB+a8VG1YfC2FlFhngme5hXo0miCVV4Fpdtcy4UvQ3DC+gK7SLg/5NoiwSI9SW6WVE6pmDrIpEar4HF+ODoinu6pgQPOVHHk0si/3mH/jmLetaQPw15LMkjYRCIFRfMqa2wDiodI7QRPlQVvu6BS6PLgfXB/KEuPtgfVcmREd7R72Mq2JZtVjvxsETNBaCGglur+WHHm/caU24uYHszXcRM2m6kdiJeQaV33I4WxLl7NFTClzaeK62ZFp/+4FcG1F2nc02F1a4abANVKzAMwJQ2EdBHg5/t4iFkNr6Uhb7NyAN5c4J58KTvEkE1Hq/qHA3MnJCKMcfkxU0cYzpZnxdx8QjpZjRveQLCXbXfOuLoAZHgvZtuHOsCeyZQL7WEZCi2BykczBNui4/Kk0GPw6fj3ABY8DHwaupWDLO19WicbekPZjUcyvJymHAuve5nPNf9Oqb6gE1zHVhKHggKPNV4SAKdNm+HtAb8bOuBnmTpaQsemC85wBNnZcFxYzyrLtU82gjcS0yBOcGcxxgL6QRhzowHrPQX1KP0Cye0vt7jCSt6/D4rYVijBZ//s6RklAQFv7/TJzwVlZSLQuLEr6+ve5lDaURdWwRe+tg3aY4VQJOLpfe1wxqBI3Z/9kM+NArVjmcoVDoXf/2wV8rDnWvnKQZjz7Bzjgfkru3PZ8dx19pH/Bsxo+Of0IRTKGJKOpDmmHu/h2oyFgnwJDYz0r65bNv6nDmHLEB/GkKSFWEDTuUNjg3oeb7fWrfGIDGxh0o1Nt/0ZpUYMVURjRo1tYWyY+utHHfySvJQagJOF477wdjWBaFnsq7BwmIVHiLUh4qcxC/8+e3GSUjsr7OaesAwmQLq4nY27zsqfZqsP94hWZbys2NZrztbqblWRVLNmp30FY/1AwA06TumNTw8UBshTT2fzF76KTYzjti70tPFkmcZU9X4q4J8fsbOfJ6FM4MYSoxaswZfms6AKX6IGIK5mf22qKY5QB+gDaQWGExzSdVJ1kq5w2Kaxs6LZaXhTW+FL9F7IjnnfbhRa8tuVoFG5xr4Wa3EpPcFOwMUaBOtM8auFuWFmXgzgRpgynw8CxAOrO2ObnmT8fWuUVGyIuh/JBciXLPaufwlQ+h/wYF2P/jP4swvJ2LzTibK633MfHD3zIMCsZYM3vqPsSol9jSmmAJ/c6ZA5LJW6ONfjg0yCPD5n6I5K/3pQf2dOm8+3arNemwFrVSx2BSbBmbT3fkt37FbHMN7o6VUrX+mh42I7sfIaB7Ee7NUh9O9GupFNgHQi0E2vZTOLNEe75or+GeietsD288Rmb+5lf2ef59SL5hqrU8bqSpKwUpBJEYSwy49L+4Kqd9ZIaSgX0nVN4Lem7+6s8U0hZihmLViKMLJEZ3GYT21TzfghNHq4XTIIp+sLX3qwckC+QraTOxUrkVXcbsGx1b0buADLAPSWUuo1fKUpmwWvVy+YZJUneRF/Ja9lKMcwSnnQIGFGIxfA2HJcmg3JTmvTNqi7lZOij5/jRni/XrAIz4oW/b6DZOO0LLgb6kPsofJVRgD/MnyjM3N1qo2nol+9ZjX3cxU9E+GCa5O5/Tfeuno96JAx3HGzMUSGl+5OnxdIYhjYBpGvjFXDDM=\")\n",
        "\n",
        "# License.SetLicenseKey(\"license.elic.xml\")\n",
        "\n",
        "####  basics ##############\n",
        "from google.colab import userdata\n",
        "\n",
        "from typing import List\n",
        "\n",
        "## for regular expressions\n",
        "import re\n",
        "\n",
        "#   import mistralai\n",
        "\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "## Define functions ################################################################\n",
        "####################################################################################\n",
        "###### PROTOTYPE ###################################################################\n",
        "AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
        "\n",
        "#######define text splitter into paragraphs (\\n\\n) and eliminate empty paragraphs\n",
        "#### NOTE WE NEED TO ADD: STRIP LEADING BLANCS!\n",
        "\n",
        "def split_text_into_paragraphs(text: str) -> List[str]:\n",
        "    \"\"\"Splits a text into paragraphs.\"\"\"\n",
        "    return [para.strip(\"\") for para in text.split('\\n\\n') if para.strip() != '']\n",
        "\n",
        "##### define function for classifying strings into question or answer using openAI\n",
        "##### needs defined llm object\n",
        "\n",
        "def classify_paragraph(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes a chat completion using the provided prompt and returns the result.\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"You are a survey researcher and your task is to classify strings as\n",
        "                \"question\", or \"answer\", or \"other\". Only answer with \"question\", \"answer\", or \"other\".\n",
        "                Learn from the following examples:\n",
        "                Would you describe yourself as the main driver of any car in your household? // question\n",
        "                Please write down all tyre brands that come to mind. Please enter only one brand in each box. // question\n",
        "                How much are you interested in buying this product? // question\n",
        "                Please specify the type of your motorcycle. // question\n",
        "                What is your age? // question\n",
        "                I would definitely buy this product // answer\n",
        "                between 15 and 24 years // answer\n",
        "                yes // answer\n",
        "                no // answer\n",
        "                At a motorcycle dealer // answer\n",
        "                don't know // answer\n",
        "                Country relevant for winter/all-season tyre // other\n",
        "                Country // other\n",
        "                System variable // other\"\"\"\n",
        "    # Create the messages list with system and user messages\n",
        "    messages = [\n",
        "        ChatMessage(role=\"system\", content=system_prompt),\n",
        "        ChatMessage(role=\"user\", content=prompt),\n",
        "    ]\n",
        "\n",
        "    # Execute the chat completion\n",
        "    try:\n",
        "       response = llm.chat(messages=messages).message.content\n",
        "    except:\n",
        "       response = \"error\"\n",
        "\n",
        "    # Extract and return the result from the response\n",
        "    return response\n",
        "\n",
        "###############################################################\n",
        "#### initialize azure openai ##################################\n",
        "###############################################################\n",
        "\n",
        "AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
        "\n",
        "aoai_api_key = AZURE_OPENAI_KEY\n",
        "aoai_endpoint = \"https://pulsar.openai.azure.com/\"\n",
        "aoai_api_version = \"2023-03-15-preview\"\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=\"gpt-4-32k\",\n",
        "    deployment_name=\"pulsar-typr-azure-openai-gpt4-32k-0613\",\n",
        "    api_key=aoai_api_key,\n",
        "    azure_endpoint=aoai_endpoint,\n",
        "    api_version=aoai_api_version,\n",
        ")\n",
        "\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    deployment_name=\"text-embedding-ada-002\",\n",
        "    api_key=aoai_api_key,\n",
        "    azure_endpoint=aoai_endpoint,\n",
        "    api_version=aoai_api_version,\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "################################################################################\n",
        "###### S E C T I O N  I ########################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "###############  INITIALISATION ################################################\n",
        "\n",
        "# all files read will be converted into standardized python dictionaries and\n",
        "# appended to so_dicts\n",
        "\n",
        "# in production, the dictionaries can be stored as jason files in sql database\n",
        "\n",
        "so_dicts = []\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################ QUALITATIVE ###################################################\n",
        "# Qualitativ - Discussin Guides for method = \"qual\"\n",
        "# Read ==> docx files from local folder\n",
        "# Using standard docx reader because no tables and want full document\n",
        "# Note: We assume that method \"qual\" is always a word document\n",
        "\n",
        "# Read files from local folder\n",
        "folder_path = \"/content/drive/MyDrive/qualdata/\"\n",
        "input_files = [\n",
        "    os.path.join(folder_path, filename)\n",
        "    for filename in os.listdir(folder_path)\n",
        "    if filename.endswith(\".docx\")\n",
        " ]\n",
        "\n",
        "# Read files with simple directory reader\n",
        "# Result is a list of document objects (llamindex document objects)\n",
        "documents = SimpleDirectoryReader(input_files=input_files).load_data()\n",
        "print(\"\\n\".join(input_files))\n",
        "len(documents)\n",
        "\n",
        "\n",
        "#  CREATE DICTIONARY FROM DOCUMENTS IN LIST OF DOCUMENTS\n",
        "# inizialize metadata for dictionary\n",
        "# NOTE: language and country are not available now\n",
        "# we need to define how country and language are manually added\n",
        "# in production version, language and country will be queried from ERP database\n",
        "\n",
        "### so_dicts = []   # for local testing only\n",
        "\n",
        "method = \"qual\"\n",
        "language = \"German\"\n",
        "country = \"Germany\"\n",
        "\n",
        "#  creat a separate dictionary from each document and append to list\n",
        "for doc in documents:\n",
        "   paragraphs = split_text_into_paragraphs(doc.text)\n",
        "   data_list =  [[string] for string in paragraphs]\n",
        "   file_name = doc.metadata[\"file_name\"]\n",
        "   file_path = doc.metadata[\"file_path\"]\n",
        "   ID = hash(file_name)\n",
        "   dict_1 = {\"ID\": ID, \"method\": method, \"language\": language, \"country\": country,\n",
        "        \"file_name\": file_name, \"file_path\": file_path,\n",
        "             \"payload\": [{\"format\": \"p\", \"content\": data_list }] }\n",
        "   so_dicts.append(dict_1)\n",
        "\n",
        "\n",
        "# experiments\n",
        "so_dicts[2][\"payload\"][0][\"content\"]\n",
        "\n",
        "len(so_dicts)\n",
        "# using list comprehension\n",
        "print([par[0] for par in so_dicts[0][\"payload\"][0][\"content\"]])\n",
        "\n",
        "print(\"\\n\\n\".join([par[0] for par in so_dicts[1][\"payload\"][0][\"content\"]]))\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "######################## QUANTIATIVE  ##########################################\n",
        "# Quantitative - Survey Questionnaires WORD   ##################################\n",
        "# for method = \"quant\"\n",
        "# Read ==> docx files from local folder\n",
        "#\n",
        "#  PROBLEM: TABLES ARE NOT PRINTED CORRECTLY FROM LLAMANINDEX DOCUMENT OBJECT\n",
        "#  SOLUTION: python-docx  OPEN SOURCE PACKAGE (REPLACES SPIRE)\n",
        "#\n",
        "####################  Extract paragraphs and tables ############################\n",
        "\n",
        "# LOOP OVER ALL TABLES, EXTRACT ROWS: 1 ROW = 1 LIST\n",
        "# LOOP OVER ALL PARAGRAPHS, EXTRACT PARAGRAPHS: 1 PARAGRAPH = 1 LIST W/ 1 ELEMENT\n",
        "\n",
        "################################################################################\n",
        "### Get input files from directory for reading docx documents\n",
        "folder_path = \"/content/drive/MyDrive/quantdata\"\n",
        "\n",
        "input_files = [filename for filename in os.listdir(folder_path) if filename.endswith(\".docx\")]\n",
        "\n",
        "\n",
        "# Load Word documents to a list of documents - SPIRE DOCUMENT OBJECT\n",
        "so_documents = []\n",
        "for file_name in input_files:\n",
        "   file_path = os.path.join(folder_path, file_name)\n",
        "   document = Document(file_path)\n",
        "   # document.LoadFromFile(file_path)\n",
        "   print(f\"\"\"sections {len(document.sections)} in document {file_name}\"\"\")\n",
        "   so_documents.append(document)\n",
        "\n",
        "\n",
        "# EXPERIMENTING\n",
        "file_path = os.path.join(folder_path, input_files[0])\n",
        "\n",
        "\n",
        "document = Document(\"/content/drive/MyDrive/quantdata/Quant_Fragebogen_Bio.docx\")\n",
        "\n",
        "print(input_files[0])\n",
        "len(so_documents)\n",
        "document = so_documents[0]\n",
        "print(document)\n",
        "\n",
        "tables = document.tables\n",
        "len(tables)\n",
        "[tables[3].rows[2].cells[i].paragraphs[0].text for i in range(len(tables[3].rows[2].cells)) ]\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# LOOPING THROUGH ALL DOCUMENTS method = \"quant\" and end \".docx\"\n",
        "# LOOPTING THROUGH ALL PARAGRAPHS AND TABLES IN ALL SECTIONS  ###\n",
        "# CREATING DICTIONARY FOR EACH document\n",
        "## append result to so_dicts = all dictionaries   ##############################\n",
        "\n",
        "#>>>>>>>>>>>>>> TO DO: LOOPING THROUGH documents\n",
        "# for testing\n",
        "## document = so_documents[0]\n",
        "\n",
        "# loop through documents\n",
        "file_index = 0   # file_index is counter = index in the loop\n",
        "for document in so_documents:\n",
        "\n",
        "  # inizialize metadata for dictionary\n",
        "  folder_path = folder_path\n",
        "  file_name = input_files[file_index]\n",
        "  file_path = os.path.join(folder_path, file_name)\n",
        "  ID = hash(file_name)\n",
        "  method = \"quant\"\n",
        "  language = \"German\"\n",
        "  country = \"Germany\"\n",
        "\n",
        "  dict_1 = {\"ID\": ID, \"method\": method, \"language\": language, \"country\": country,\n",
        "        \"file_name\": file_name, \"file_path\": file_path }\n",
        "\n",
        "  dict_1[\"payload\"] = []\n",
        "\n",
        "      # get the paragraphs in the document\n",
        "      # create list of paragraphs - each paragraphs is a list in itself\n",
        "      # one Text string per paragraph = list\n",
        "      # all paragraphs of a section in the data_list\n",
        "  data_list = []\n",
        "\n",
        "      # loop through the paragraphs of the document\n",
        "  for i in range(len(document.paragraphs)):\n",
        "         if document.paragraphs[i].text != '':\n",
        "            data_list.append([document.paragraphs[i].text])\n",
        "  dict_1[\"payload\"].append({\"format\": \"p\", \"content\":  data_list})\n",
        "\n",
        "\n",
        "      # Get the tables in the document\n",
        "      # each table is a seaparate data_list\n",
        "      # table rows are list of strings\n",
        "  tables = document.tables\n",
        "\n",
        "      # Loop through the tables\n",
        "  for i in range(0, len(tables)):\n",
        "\n",
        "          ## do one table only\n",
        "          table = table = tables[i]\n",
        "          # Create a list to store the extracted table data\n",
        "          data_list = []\n",
        "          data_row = []\n",
        "\n",
        "          # Loop through the rows in the table\n",
        "          for j in range(len(table.rows)):\n",
        "             row = table.rows[j]\n",
        "             data_row = []\n",
        "             # Loop through the cells in each row\n",
        "             for k in range(len(row.cells)):\n",
        "                cell = row.cells[k]\n",
        "                # Loop through the paragraphs in each cell\n",
        "                cell_text = \"\"\n",
        "                for p in range(len(cell.paragraphs)):\n",
        "                   # Extract data from each paragraph\n",
        "                   paragraph = cell.paragraphs[p]\n",
        "                   text = paragraph.text\n",
        "                   # print(f\"\"\"{k} ... {text}...\"\"\")\n",
        "                   # Append the data to the list\n",
        "                   cell_text = cell_text + text\n",
        "\n",
        "                # print(cell_text)\n",
        "                # print(f\"\"\"{i} ... {cell_text}...\"\"\")\n",
        "                data_row.append(cell_text)\n",
        "             print(f\"\"\"{i} ... {data_row}...\"\"\")\n",
        "             data_list.append(data_row)\n",
        "          dict_1[\"payload\"].append({\"format\": \"t\", \"content\":  data_list})\n",
        "\n",
        "  so_dicts.append(dict_1)\n",
        "  file_index = file_index + 1\n",
        "\n",
        "\n",
        "### EXPERIMENTING\n",
        "len(so_dicts)\n",
        "so_dicts[6][\"ID\"]\n",
        "## dict_1[\"payload\"].append({\"format\": \"t\", \"content\":  data_list})\n",
        "dict_1\n",
        "document.Close()\n",
        "\n",
        "dict_1[\"file_name\"]\n",
        "so_dicts[4][\"payload\"][8]\n",
        "\n",
        "## nice print of payload paragraphs and tables\n",
        "dict_1[\"payload\"][1][\"format\"]\n",
        "print(\" \\n\".join(so_dicts[4][\"payload\"][8][\"content\"][0]))\n",
        "for s in so_dicts[3][\"payload\"][8][\"content\"]:\n",
        "    print(\"\\t¦\".join(s))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "### Quantitative - Survey Questionnaires EXCEL\n",
        "### Read ==> xlsx files from local folder\n",
        "####### WITH A DICTIONARY FOR EACH FILE  AS FINAL OUTPUT   #####################\n",
        "\n",
        "### Get input files from directory for reading docx documents\n",
        "folder_path = \"/content/drive/MyDrive/quantdata\"\n",
        "\n",
        "input_files = [filename for filename in os.listdir(folder_path)\n",
        "  if filename.endswith(\".xlsx\")  or filename.endswith('.xls')]\n",
        "print(\"\\n\".join(input_files))\n",
        "\n",
        "#######  EXTRACT INTO 1  DICTIONARY PER XLS DOCUMENT\n",
        "#######  LOOP THROUGH ALL XLS FILES AND CREATE ONE DICTIONARY FOR EACH XLS FILE\n",
        "#######  EACH ROW OF THE EXCEL IS ONE LIST IN A LIST = CONTENT\n",
        "# file_name = input_files[2]\n",
        "#min([df.shape[1],15])\n",
        "# df.shape[1]\n",
        "\n",
        "## so_dicts = []   # only for local testing of loops\n",
        "method = \"quant\"\n",
        "language = \"German\"\n",
        "country = \"Germany\"\n",
        "\n",
        "for file_name in input_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    ID = hash(file_name)\n",
        "    # Read  the Excel file into a DataFrame and extract rows as string\n",
        "    df = pd.read_excel(file_path, engine='openpyxl', dtype=str)\n",
        "    ### identify first column with question texts up to last column or column 15\n",
        "    soq = []\n",
        "    for i in range(10,40):\n",
        "      for j in range(min([df.shape[1],15])):\n",
        "        if len(str(df.iloc[i,j])) > 10:\n",
        "          if \"question\" == classify_paragraph(df.iloc[i,j]):\n",
        "              soq.append(j)\n",
        "              print(j)\n",
        "              print(df.iloc[i,j])\n",
        "    if not soq: soq.append[11]\n",
        "    print(min(soq))\n",
        "    # replace NaN with string = blanc\n",
        "    df.fillna('', inplace=True)\n",
        "    #shorten df columns up to  first column with question text found\n",
        "    dfs = df.iloc[:,:min(soq)+1]\n",
        "\n",
        "    # add dataframe to list of dataframes OBSOLETE - USE DICTIONARIES\n",
        "    # so_dataframes.append(dfs)\n",
        "    # add dictionary to list of dictionaries\n",
        "    dicts = {\"ID\": ID, \"method\": method, \"language\": language, \"country\": country,\n",
        "                \"file_name\": file_name, \"file_path\": file_path,\n",
        "                 \"payload\": [{\"format\": \"x\", \"content\":  dfs.values.tolist()}] }\n",
        "    so_dicts.append(dicts)\n",
        "\n",
        "\n",
        "# EXPERIMENTING\n",
        "len(so_dicts)\n",
        "dict_1 = so_dicts[7]\n",
        "dict_1[\"ID\"]\n",
        "dict_1[\"method\"]\n",
        "dict_1[\"payload\"][0][\"format\"]\n",
        "dict_1[\"payload\"][0][\"content\"]\n",
        "\n",
        "for s in so_dicts[7][\"payload\"][0][\"content\"]:\n",
        "    print(\"\\t¦\".join(s))\n",
        "\n",
        "dfs.iloc[0:40,:]\n",
        "df.iloc[0:20,0:13]\n",
        "# EXPERIMENTING\n",
        "len(str(df.iloc[10,9]))\n",
        "classify_paragraph(df.iloc[10,9])\n",
        "type(df.iloc[10,9])\n",
        "\n",
        "len(so_dataframes)\n",
        "df_1 = so_dataframes[2]\n",
        "df_1.shape[1]\n",
        "\n",
        "df_1.iloc[0:40,0:11]\n",
        "so_listx = df_1.values.tolist()\n",
        "so_listx[28][9]\n",
        "type(so_listx)\n",
        "len(so_listx)\n",
        "\n",
        "\n",
        "classify_paragraph(so_listx[28][9])\n",
        "\n",
        "type(df_1)\n",
        "\n",
        "################################################################################\n",
        "###### S E C T I O N  II #######################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "## EXTRACTING STRINGS AND ID FROM DICTIONARIES AS INPUT FOR EMBEDDING ##########\n",
        "\n",
        "## INITIALIZING THE LISTS FOR INPUT DATA FOR EMBEDDINGS INCL. METADATA\n",
        "\n",
        "d_list = []    # document ID\n",
        "p_list = []    # payload index\n",
        "q_list = []    # element = question or paragraph text to be embedded\n",
        "r_list = []    # element = row of question text in payload p\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "##  EXTRACTION OF all PARAGRAPHS FROM DICTIONARIES CONTENT FOR METHOD = \"qual\" #\n",
        "## for DICTIONARIES: extract all paragraphs  ###################################\n",
        "## works for word files read with simple paragraph splitter ####################\n",
        "\n",
        "# experimenting\n",
        "## if work with one dictionary only extract it from so_dicts\n",
        "dict_1 = so_dicts[0]\n",
        "len(so_dicts)\n",
        "# experimenting\n",
        "data_list = dict_1[\"payload\"][0][\"content\"]\n",
        "dict_1[\"payload\"][0][\"format\"]\n",
        "dict_1[\"payload\"][0][\"content\"]\n",
        "\n",
        "################################################################################\n",
        "######  initalize loop  for qual dictionaries only  ############################\n",
        "##  remove initialization here for full run over qual and quant ################\n",
        "# only for local testing\n",
        "##d_list = []    # document ID\n",
        "#p_list = []    # payload index\n",
        "#q_list = []    # element = question text\n",
        "#r_list = []    # element = row of question text in payload p\n",
        "\n",
        "\n",
        "#### loop through the dictionaries of so_dicts  of method = \"qual\"\n",
        "for dict_1 in [i for (i, v) in zip(so_dicts, [dic[\"method\"] == \"qual\" for dic in so_dicts]) if v]:\n",
        "\n",
        "##### loop through el = elements of list of lists = rows in payload\n",
        "\n",
        "  p=0\n",
        "  for el in dict_1[\"payload\"]:\n",
        "    print(p)\n",
        "    print(dict_1[\"file_name\"])\n",
        "    print(el[\"format\"])\n",
        "    # flatten q_list into list of strings and extend q_list\n",
        "    [q_list.extend(sublist) for sublist in el[\"content\"]]\n",
        "    r_list.extend([x for x in range(0,len(el[\"content\"]))])\n",
        "    p_list.extend([p] *len(el[\"content\"]) )\n",
        "    d_list.extend([dict_1[\"ID\"]] * len(el[\"content\"]))\n",
        "    print(dict_1[\"payload\"][p][\"content\"])\n",
        "    p = p+1\n",
        "\n",
        "\n",
        "## experiment\n",
        "\n",
        "flat_list=[]\n",
        "[flat_list.extend(sublist) for sublist in elx[\"content\"]]\n",
        "len(flat_list)\n",
        "\n",
        "len(q_list)\n",
        "len(r_list)\n",
        "len(p_list)\n",
        "len(d_list)\n",
        "\n",
        "mylist = [x for x in range(0,53)]\n",
        "[1] * 5\n",
        "elx= dict_1[\"payload\"][0]\n",
        "mylist2 = [p] * len(elx[\"content\"])\n",
        "len(mylist2)\n",
        "mylist3 = [dict_1[\"ID\"]] * len(elx[\"content\"])\n",
        "len(mylist3)\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "##  EXTRACTION OF QUESTIONS FROM DICTIONARIES FOR METHOD = \"quant\" #############\n",
        "## for DICTIONARIES: extract rows marked as \"question\" in same row #############\n",
        "## works for excel files and for tables from word file sections ################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "### extract dictionaries with method = \"quant\" #################################\n",
        "\n",
        "# so_dicts_quant = [i for (i, v) in zip(so_dicts, [dic[\"method\"] == \"quant\" for dic in so_dicts]) if v]\n",
        "len(so_dicts)\n",
        "\n",
        "# for local testing of loops only\n",
        "#d_list = []    # document ID\n",
        "#p_list = []    # payload index\n",
        "#q_list = []    # element = question text\n",
        "#r_list = []    # element = row of parapraph text in payload p\n",
        "\n",
        "\n",
        "##### loop through dictionaries with method = quant\n",
        "\n",
        "for dict_1 in [i for (i, v) in zip(so_dicts, [dic[\"method\"] == \"quant\" for dic in so_dicts]) if v]:\n",
        "\n",
        "##### loop through el = elements of list of lists = rows in payload\n",
        "  p=0\n",
        "  print(dict_1[\"file_name\"])\n",
        "  for el in dict_1[\"payload\"]:\n",
        "    q_len = 0\n",
        "    print(p)\n",
        "    print(el[\"format\"])\n",
        "    if el[\"format\"] == \"t\" or el[\"format\"] == \"x\":\n",
        "      data_list = el[\"content\"]\n",
        "\n",
        "      for i in range(len(data_list)):\n",
        "        q_ind = False   # initialize - indicating a row with \"question\" or \"q\"\n",
        "        row = list(data_list[i])\n",
        "        if \"question\" in [x.lower() for x in row]:\n",
        "          row.pop([x.lower() for x in row].index(\"question\"))\n",
        "          q_ind = True\n",
        "        if \"q\" in [x.lower() for x in row]:\n",
        "          row.pop([x.lower() for x in row].index(\"q\"))\n",
        "          q_ind = True\n",
        "        q_success = False  # initializie - indicating wether row is appended\n",
        "          ### row[-1] ONLY FOR FORMAT X // DO NOT INSERT SPACE IF row[-1]\n",
        "        if el[\"format\"] == \"x\" and q_ind and row[-1].strip() != \"\":\n",
        "           q_list.append(\"\".join(row[-1]))\n",
        "           q_success = True\n",
        "        elif el[\"format\"] == \"t\" and q_ind:\n",
        "           q_list.append(\" \".join(row))\n",
        "           q_success = True\n",
        "        else:\n",
        "          pass\n",
        "          #print(f'{i}The string question was not found in the list.')\n",
        "        if q_success:\n",
        "             q_len = q_len + 1\n",
        "             r_list.append(i-0)\n",
        "             p_list.append(p)\n",
        "             d_list.append(dict_1[\"ID\"])\n",
        "             print(dict_1[\"payload\"][p][\"content\"][i])\n",
        "\n",
        "  ###   fallback if no \"q\" or \"question\"  found = q_len = 0\n",
        "  ###   all rows with blanc in row left to last row with questions\n",
        "  ###   needs improvment  metadata q_found = \"yes\" if last row is question\n",
        "      if el[\"format\"] == \"x\" and q_len < 1:\n",
        "         for i in range(len(data_list)):\n",
        "            row = list(data_list[i])\n",
        "            if bool(re.fullmatch(\"\",row[-2])) and \"\".join(row[-1]) != \"\":\n",
        "              q_list.append(\"\".join(row[-1]))\n",
        "              r_list.append(i-0)\n",
        "              p_list.append(p)\n",
        "              d_list.append(dict_1[\"ID\"])\n",
        "              print(dict_1[\"payload\"][p][\"content\"][i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    p=p+1\n",
        "\n",
        "\n",
        "# EXPERIMENTING\n",
        "q_len\n",
        "q_list\n",
        "len(d_list)\n",
        "len(p_list)\n",
        "len(q_list)\n",
        "len(r_list)\n",
        "\n",
        "print(q_list)\n",
        "print(data_list[r_list[21]])\n",
        "print(\"\\n\".join(q_list))\n",
        "str(data_list[0])\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "## Experiments only.\n",
        "# The function classify_paragraph() is created at start of script\n",
        "# Classify paragraph question, answer, other\n",
        "\n",
        "# Experiments\n",
        "\n",
        "prompt = '50-59 Jahre'\n",
        "prompt = '60-69 Jahre'\n",
        "prompt = '70 Jahre oder älter'\n",
        "prompt = 'TOPIC'\n",
        "prompt = 'Region'\n",
        "prompt = \"Bitte geben Sie nun an, welche der folgenden Supermärkte Sie kennen, wenn auch nur dem Namen nach.\"\n",
        "prompt = \"Aldi\"\n",
        "prompt = \"Alnatura\"\n",
        "prompt = \"Wer ist bei Ihnen zu Hause für den Kauf von Lebensmitteln zuständig?\"\n",
        "prompt = \"Ich gemeinsam mit anderen Personen (Partner, Eltern, Mitbewohner,…)\"\n",
        "prompt = \"Andere Personen (Partner, Eltern, Mitbewohner,...)\"\n",
        "prompt =  \"Gestützte Bekanntheit Einkaufsstätten\"\n",
        "\n",
        "classify_paragraph(prompt)\n",
        "\n",
        "\n",
        "## split text experiments\n",
        "text_list = [\"\"\"How much do you like this product?\"\"\",\"\"\"rather dislike\"\"\",\"\"\"Ich gemeinsam mit anderen Personen (Partner, Eltern, Mitbewohner,…)\"\"\"]\n",
        "so_documents = [Document(text=t) for t in text_list]\n",
        "\n",
        "para_class = []\n",
        "## the split_text_into_paragraphs() function is not loaded - c.f. code collection below\n",
        "for doc in so_documents:\n",
        "  paragraphs = split_text_into_paragraphs(doc.text)\n",
        "  for par in paragraphs:\n",
        "     para_class.append(classify_paragraph(par))\n",
        "\n",
        "################################################################################\n",
        "###### S E C T I O N  III  #####################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "###### ADDING METADATA AND EMBEDDING RELEVANT TEXT STRINGS  ####################\n",
        "###\n",
        "### Preparing two auxiliary dataframes for easy access to metadata and content\n",
        "###\n",
        "### df_documents: document ID and all dictionaries in one column (= series)\n",
        "### the dictionaries are contained in the dataframe series = column \"Dict\"\n",
        "### in df_documents the metadata is \"hidden\" in the dictionaries in \"Dict\"\n",
        "### in production we will probably store the dictionaries as jason in SQL\n",
        "\n",
        "df_documents =  pd.DataFrame({\"DocumentID\":[dict[\"ID\"] for dict in so_dicts],\n",
        "                              \"Dict\": so_dicts\n",
        "                             })\n",
        "## inspect result\n",
        "df_documents[\"DocumentID\"]\n",
        "\n",
        "#############  write json of documents dictionary  #############################\n",
        "jason_documents = {\"DocumentID\":[dict[\"ID\"] for dict in so_dicts],\n",
        "                              \"Dict\": so_dicts\n",
        "                             }\n",
        "## FOR WRITING DOCUMENTS AS JASON CREATE JUST A DICTIONARY\n",
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/probing_data/jason_documents2.json\", \"wb\") as f:\n",
        "    f.write(json.dumps(jason_documents).encode(\"utf-8\"))\n",
        "\n",
        "## df_metadata is redundant, but it allows an easy merge with dataframe df_index\n",
        "## in production this will probably be taken from an squl database\n",
        "\n",
        "df_metadata = pd.DataFrame({\"DocumentID\":[dict[\"ID\"] for dict in so_dicts],\n",
        "                            \"method\": [dict[\"method\"] for dict in so_dicts],\n",
        "                            \"country\":  [dict[\"country\"] for dict in so_dicts],\n",
        "                            \"language\":  [dict[\"language\"] for dict in so_dicts]\n",
        "                             })\n",
        "\n",
        "################################################################################\n",
        "# Generate embeddings and store in vectors list and then add to df_index\n",
        "\n",
        "# Store  document information\n",
        "## doc_titles = [] #obsolete\n",
        "# d_list = []    # document ID\n",
        "# p_list = []    # payload index\n",
        "# q_list = []    # element = question or paragraph text\n",
        "# r_list = []    # element = row of question text in payload p\n",
        "\n",
        "## initialize df_index with metadata and input strings for embedding\n",
        "## column with vectors will be added after embedding process is completed\n",
        "\n",
        "df_index = pd.merge(\n",
        "                    pd.DataFrame({'DocumentID' : d_list,\n",
        "                         'PayloadIdx' : p_list,\n",
        "                         'RowIdx' : r_list ,\n",
        "                         'Text' : q_list\n",
        "                         }) ,\n",
        "                    df_metadata,\n",
        "                   left_on='DocumentID', right_on=\"DocumentID\", how='left')\n",
        "\n",
        "len(df_index)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###  FILTER  - THIS WILL BE A DROPDOWN IN THE PROTOYPE APP\n",
        "###  FILTERS ARE: method (quant/ qual), country, language dbd.\n",
        "#### for this script:  Filtering lists by metadata. example: method\n",
        "###  Final solution will use SQL query to data in dictionaries (??)\n",
        "\n",
        "###  EXAMPLE FOR FILTERING:\n",
        "## df_index = df_index.loc[df_index[\"method\"] == \"quant\",:]\n",
        "\n",
        "################################################################################\n",
        "### CREATING EMBEDDINGS =  INDEX VECTORS  ######################################\n",
        "vectors = []\n",
        "vec_error = embed_model.get_text_embedding(\"error\")\n",
        "\n",
        "df_index[\"Text\"][0]\n",
        "\n",
        "for q in df_index[\"Text\"]:\n",
        "  try:\n",
        "   vector = embed_model.get_text_embedding(q)\n",
        "   print(vector[0:4])\n",
        "   vectors.append(vector)\n",
        "  except:\n",
        "   vectors.append(vec_error)\n",
        "   print(\"error\" + q)\n",
        "\n",
        "df_index[\"Vector\"] = vectors\n",
        "\n",
        "\n",
        "#######  WRITE df_index TO JASON FILE ON DRIVE #################################\n",
        "\n",
        "df_index_as_json_str = df_index.to_json(orient='records')\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/probing_data/df_index_as_json_str2.json\", \"wb\") as f:\n",
        "   f.write(json.dumps(df_index_as_json_str).encode(\"utf-8\"))\n",
        "\n",
        "\n",
        "\n",
        "#EXPERIMENTING\n",
        "len(vectors)\n",
        "q_list[q_list == \"\"]\n",
        "\n",
        "q_list.index(\"\")\n",
        "q_list[0]\n",
        "q_list\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###### S E C T I O N  IV  ######################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "### QUERY  #####################################################################\n",
        "\n",
        "\n",
        "query = \"Alles in allem, wie zufrieden sind Sie insgesamt mit der Suva? \"\n",
        "query = \"Do you currently own and run a motorcycle?\"\n",
        "query = \"Anforderungen an Wassersprudler\"\n",
        "query = \"Warum kaufen Sie die Marke Tesa?\"\n",
        "\n",
        "query_vector = embed_model.get_text_embedding(query)\n",
        "\n",
        "df = df_index\n",
        "\n",
        "# Calculate the similarity between the query vector and the document vectors\n",
        "df[\"Similarity\"] = df[\"Vector\"].apply(\n",
        "        lambda x: cosine_similarity([x], [query_vector])[0][0]\n",
        "    )\n",
        "\n",
        "# Sort the DataFrame by similarity in descending order\n",
        "df = df.sort_values(by=\"Similarity\", ascending=False)\n",
        "\n",
        "# Get the top 3 most similar documents\n",
        "df_top_documents = df.head(3)\n",
        "\n",
        "## EXPERIMENT\n",
        "df_top_documents[\"DocumentID\"]\n",
        "print(df_top_documents[\"Text\"].iloc[0])\n",
        "print(df_top_documents[\"Text\"].iloc[1])\n",
        "print(df_top_documents[\"Text\"].iloc[2])\n",
        "\n",
        "### lookup of dictionary information for the top results - now with dataframe but sql in future\n",
        "### df_documents contains the original document dictionaries that could be stored as json in SQL\n",
        "## add document information from df_documents to df_top_documents\n",
        "df_merged = pd.merge(df_top_documents, df_documents, left_on='DocumentID', right_on='DocumentID', how='left')\n",
        "len(df_merged)\n",
        "\n",
        "################################################################################\n",
        "# PRINTING OUTPUT ##############################################################\n",
        "## extract original information from merged dataframe\n",
        "## extract information from document's dictionary\n",
        "\n",
        "for rslt in range(0,3):\n",
        "  content = df_merged[\"Dict\"][rslt][\"payload\"][df_merged[\"PayloadIdx\"].iloc[rslt]][\"content\"]\n",
        "  print(\"Original document:\\n\",\n",
        "    \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]]),\"\\n\",\n",
        "    \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]+1]),\"\\n\",\n",
        "    \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]+2]),\n",
        "    \"\\n\\nfile_name:\",\n",
        "    df_merged[\"Dict\"][rslt][\"file_name\"],\n",
        "    \"\\nmethod:\",\n",
        "    df_merged[\"Dict\"][rslt][\"method\"],\n",
        "    \"\\nlink:\",\n",
        "    df_merged[\"Dict\"][rslt][\"file_path\"],\n",
        "    \"\\n\\n****************************************************************************\\n\\n\")\n",
        "\n",
        "\n",
        "[i for i in content]\n",
        "\n",
        "test_list = sum(content, [])\n",
        "\n",
        "# EXPERIMENTING\n",
        "## extract original information from merged dataframe\n",
        "## extract information from document's dictionary\n",
        "\n",
        "for rslt in range(0,3):\n",
        "  print(\"Original document:\\n\",\n",
        "    \"¦\\t\".join(df_merged[\"Dict\"][rslt][\"payload\"][df_merged[\"PayloadIdx\"].iloc[rslt]][\"content\"][df_merged[\"RowIdx\"].iloc[rslt]:df_merged[\"RowIdx\"].iloc[rslt]+5]\n",
        "    ),\n",
        "    \"\\n\\nfile_name:\",\n",
        "    df_merged[\"Dict\"][rslt][\"file_name\"],\n",
        "    \"\\nmethod:\",\n",
        "    df_merged[\"Dict\"][rslt][\"method\"],\n",
        "    \"\\nlink:\",\n",
        "    df_merged[\"Dict\"][rslt][\"file_path\"],\n",
        "    \"\\n\\n****************************************************************************\\n\\n\")\n",
        "\n",
        "\n",
        "## EXPERIMENT\n",
        "\n",
        "df_merged[\"Dict\"][0][\"payload\"][df_merged[\"PayloadIdx\"].iloc[0]][\"content\"]\n",
        "[i for i in range(0,3)]\n",
        "print(\"Original document:\\n\",\n",
        "   \"¦\\t\".join(df_merged[\"Dict\"][0][\"payload\"][df_merged[\"PayloadIdx\"].iloc[0]]\n",
        "             [\"content\"][df_merged[\"RowIdx\"].iloc[0]]\n",
        "   ),\n",
        "   \"\\nfile_name:\",\n",
        "   df_merged[\"Dict\"][0][\"file_name\"],\n",
        "   \"\\nmethod:\",\n",
        "   df_merged[\"Dict\"][0][\"method\"],\n",
        "   \"\\nlink:\",\n",
        "   df_merged[\"Dict\"][0][\"file_path\"]  )\n",
        "\n",
        "\n",
        "\n",
        "df_merged[\"Dict\"][0][\"payload\"][0][\"content\"][20:25]\n",
        "\n",
        "type(df_merged[\"Dict\"][0][\"payload\"][0][\"content\"])\n",
        "\n",
        "len(df_merged)\n",
        "\n",
        "df_top_documents[\"PayloadIdx\"].iloc[0]\n",
        "df_top_documents[\"RowIdx\"].iloc[0]\n",
        "\n",
        "type(df_merged[\"Dict\"])\n",
        "\n",
        "print(df_merged[[\"Text\",\"Document Link\"]].to_string(index=False))\n",
        "\n",
        "## extract original information from merged dataframe - extract information from document's dictionary\n",
        "print(\"Original document:\\n\\n\",\n",
        "   \"¦\\t\".join(df_merged[\"Dict\"][0][\"payload\"][df_top_documents[\"PayloadIdx\"].iloc[0]][\"content\"][df_top_documents[\"RowIdx\"].iloc[0]]\n",
        "   ),\n",
        "   \"\\nfile_name:\",\n",
        "   df_merged[\"Dict\"][0][\"file_name\"],\n",
        "   \"\\nmethod:\",\n",
        "   df_merged[\"Dict\"][0][\"method\"],\n",
        "   \"\\nlink:\",\n",
        "   df_merged[\"Dict\"][0][\"file_path\"]  )\n",
        "############ LEARNINGS FOR QDRANT SOLUTION\n",
        "\n",
        "# The df_paragraphs dataframe is to be stored in the qdrant\n",
        "# The Document Id may be used as ID in the quadrant collection\n",
        "# The Metadata for the document can be stored in a SQL database.\n",
        "# The Metadata for the 3 top results is queried from the SQL database via Document ID\n",
        "# Important: We need to filter the search in the qdrant collection by relevant metadata:\n",
        "# examples: type of document (guide, questionnaire), country, language\n",
        "# The metadata that is used for filters must be stored in the payload of the qdrant collection\n",
        "\n",
        "\n",
        "###### END ########################################################\n",
        "\n",
        "first row of the dataframe: content[df_merged[\"RowIdx\"].iloc[rslt]]\n",
        "second row of the dataframe: content[df_merged[\"RowIdx\"].iloc[rslt]+1]\n",
        "third row of the dataframe: content[df_merged[\"RowIdx\"].iloc[rslt]+2]\n",
        "\n",
        "tenth row of the dataframe: content[df_merged[\"RowIdx\"].iloc[rslt]+9]\n",
        "\n",
        "\n",
        "for i in\n",
        "\n",
        "\n",
        " \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]]),\"\\n\",\n",
        "    \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]+1]),\"\\n\",\n",
        "    \"¦\\t\".join(content[df_merged[\"RowIdx\"].iloc[rslt]+2]),\n"
      ],
      "metadata": {
        "id": "NpqWCK40MnNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe6f86b-974a-4c31-e466-65fc3faa89b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SEMANTIC SPLITTER**\n"
      ],
      "metadata": {
        "id": "J_1R8inz2x0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########  EXPERIMENTING WITH SEMANTIC SPLITTING\n",
        "\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.text_splitter import SemanticTextSplitter\n",
        "\n",
        "from llama_index.core.node_parser import (\n",
        "    SentenceSplitter,\n",
        "    SemanticSplitterNodeParser,\n",
        "  )\n",
        "\n",
        "splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1, breakpoint_percentile_threshold=80, embed_model=embed_model\n",
        "  )\n",
        "\n",
        "# also baseline splitter\n",
        "base_splitter = SentenceSplitter(chunk_size=512)\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "\n",
        "print(nodes[3].get_content())\n",
        "\n",
        "for idx, node in enumerate(nodes):\n",
        "    print(f\"Chunk {idx+1}:\")\n",
        "    print(node.text)\n",
        "    print(\"=\"*50)  # Separator for readability"
      ],
      "metadata": {
        "id": "83VdOm5e247q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function that determines wether a cell contains only an integer**"
      ],
      "metadata": {
        "id": "3tVM7w7LmPpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### determine paragraphs which contain only an integer = answer code\n",
        "\n",
        "def contains_only_integer(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Identifies if the input string contains only an integer (positive or negative) and nothing else.\n",
        "\n",
        "    :param text: The input string to check.\n",
        "    :return: True if the string contains exactly an integer and nothing else, False otherwise.\n",
        "    \"\"\"\n",
        "    # Regular expression to match a whole integer (optional negative sign)\n",
        "    pattern = r'^-?\\d+$'\n",
        "\n",
        "    # Return True if the entire string matches the pattern (contains only an integer)\n",
        "    return bool(re.fullmatch(pattern, text))\n"
      ],
      "metadata": {
        "id": "ISMSWGKZmUb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting Drive**"
      ],
      "metadata": {
        "id": "vXaQ5xSi3On8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EUAvQ68AM3FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chatcompletion with the llamandiex setting.llm = llm**"
      ],
      "metadata": {
        "id": "MVkBTnRYtKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Who is the president of the United States of America\"\"\"\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are friendly assistant\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=prompt),\n",
        "]\n",
        "\n",
        "response = llm.chat(messages=messages)\n",
        "\n",
        "response.message.content\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "HR8G1OBhtZOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The code used in Typr for using the azure api chat completion**"
      ],
      "metadata": {
        "id": "44S9U4LWdOVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###########  this is the code used by typr.ai ####################\n",
        "\n",
        "chat_completion_args = {\n",
        "    \"model\": \"gpt-4\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the future of AI in healthcare?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 100\n",
        "   }\n",
        "\n",
        "response = client_azure_openai.chat.completions.create(**chat_completion_args)"
      ],
      "metadata": {
        "id": "mvvd3v7Gdaij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading table in a word file**"
      ],
      "metadata": {
        "id": "SMhbljt9H16v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spire.doc import *\n",
        "from spire.doc.common import *\n",
        "\n",
        "# Create a Document instance\n",
        "document = Document()\n",
        "# Load a Word document\n",
        "document.LoadFromFile(\"CreateTable.docx\")\n",
        "\n",
        "# Get the first section of the document\n",
        "section = document.Sections[0]\n",
        "\n",
        "# Get the first table in the section\n",
        "table = section.Tables[0]\n",
        "\n",
        "# Create a list to store the extracted table data\n",
        "data_list = []\n",
        "\n",
        "# Loop through the rows in the table\n",
        "for i in range(table.Rows.Count):\n",
        "    row = table.Rows[i]\n",
        "    # Loop through the cells in each row\n",
        "    for j in range(row.Cells.Count):\n",
        "        cell = row.Cells[j]\n",
        "        # Loop through the paragraphs in each cell\n",
        "        for k in range(cell.Paragraphs.Count):\n",
        "            # Extract data from each paragraph\n",
        "            paragraph = cell.Paragraphs[k]\n",
        "            text = paragraph.Text\n",
        "            # Append the data to the list\n",
        "            data_list.append(text + \"\\t\")\n",
        "    data_list.append(\"\\n\")\n",
        "\n",
        "# Write the extracted data into a text file\n",
        "with open(\"TableData.txt\", \"w\", encoding = \"utf-8\") as text_file:\n",
        "    for data in data_list:\n",
        "        text_file.write(data)\n",
        "\n",
        "document.Close()"
      ],
      "metadata": {
        "id": "WHSQ_KTgH7Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full loops extract tables from docx document**"
      ],
      "metadata": {
        "id": "-Qpl8ycw_KeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spire.doc import *\n",
        "\n",
        "from spire.doc.common import *\n",
        "\n",
        "# Create an instance of Document\n",
        "\n",
        "doc = Document()\n",
        "\n",
        "# Load a Word document\n",
        "\n",
        "doc.LoadFromFile(\"Sample.docx\")\n",
        "\n",
        "# Loop through the sections\n",
        "\n",
        "for s in range(doc.Sections.Count):\n",
        "\n",
        "    # Get a section\n",
        "\n",
        "    section = doc.Sections.get_Item(s)\n",
        "\n",
        "    # Get the tables in the section\n",
        "\n",
        "    tables = section.Tables\n",
        "    # Loop through the tables\n",
        "\n",
        "    for i in range(0, tables.Count):\n",
        "\n",
        "        # Get a table\n",
        "\n",
        "        table = tables.get_Item(i)\n",
        "\n",
        "        # Initialize a string to store the table data\n",
        "\n",
        "        tableData = ''\n",
        "\n",
        "        # Loop through the rows of the table\n",
        "\n",
        "        for j in range(0, table.Rows.Count):\n",
        "\n",
        "            # Loop through the cells of the row\n",
        "\n",
        "            for k in range(0, table.Rows.get_Item(j).Cells.Count):\n",
        "\n",
        "                # Get a cell\n",
        "\n",
        "                cell = table.Rows.get_Item(j).Cells.get_Item(k)\n",
        "\n",
        "                # Get the text in the cell\n",
        "\n",
        "                cellText = ''\n",
        "\n",
        "                for para in range(cell.Paragraphs.Count):\n",
        "\n",
        "                    paragraphText = cell.Paragraphs.get_Item(para).Text\n",
        "\n",
        "                    cellText += (paragraphText + ' ')\n",
        "\n",
        "                # Add the text to the string\n",
        "\n",
        "                tableData += cellText\n",
        "\n",
        "                if k < table.Rows.get_Item(j).Cells.Count - 1:\n",
        "\n",
        "                    tableData += '\\t'\n",
        "\n",
        "            # Add a new line\n",
        "\n",
        "            tableData += '\\n'\n",
        "\n",
        "\n",
        "\n",
        "        # Save the table data to a text file\n",
        "\n",
        "        with open(f'output/Tables/WordTable_{s+1}_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
        "\n",
        "            f.write(tableData)\n",
        "\n",
        "doc.Close()\n"
      ],
      "metadata": {
        "id": "hqbA2hXm_Pae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification of question, answer, other with llm api**"
      ],
      "metadata": {
        "id": "2WLj3cYCB0Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### LAMAINDEX: USING THE CHATCOMPLETION WITH AZURE openAI model #############\n",
        "###### PURPOSE: CLASSIFICATION OF QUANT SURVEY TEXTS AS Q OR A #################\n",
        "\n",
        "\n",
        "system_prompt = \"\"\"You are a survey researcher and your task is to classify strings as\n",
        "                \"question\", or \"answer\", or \"other\". Only answer with \"question\", \"answer\", or \"other\".\n",
        "                Learn from the following examples:\n",
        "                Would you describe yourself as the main driver of any car in your household? // question\n",
        "                Please write down all tyre brands that come to mind. Please enter only one brand in each box. // question\n",
        "                How much are you interested in buying this product? // question\n",
        "                Please specify the type of your motorcycle. // question\n",
        "                What is your age? // question\n",
        "                Why did you purchase these tyres? Please select all reasons that apply. // question\n",
        "                I would definitely buy this product // answer\n",
        "                between 15 and 24 years // answer\n",
        "                yes // answer\n",
        "                no // answer\n",
        "                At a motorcycle dealer // answer\n",
        "                don't know // answer\"\"\"\n",
        "\n",
        "\n",
        "prompt = \"\"\"How much do you like this product?\"\"\"\n",
        "prompt = \"\"\"yes\"\"\"\n",
        "prompt = \"\"\"rather dislike\"\"\"\n",
        "prompt = \"\"\"in the internet\"\"\"\n",
        "prompt = \"\"\"Where did you look for information about healthy food?\"\"\"\n",
        "prompt = \"\"\"Ich gemeinsam mit anderen Personen (Partner, Eltern, Mitbewohner,…)\"\"\"\n",
        "\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=system_prompt),\n",
        "    ChatMessage(role=\"user\", content=prompt),\n",
        "   ]\n",
        "\n",
        "response = llm.chat(messages=messages).message.content\n",
        "\n",
        "result = response.message.content\n",
        "\n",
        "response.raw.id\n",
        "\n",
        "response.raw.usage.total_tokens\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "YEF65SgOB60k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experimenting with excel and type document**"
      ],
      "metadata": {
        "id": "AtzWqWq2ly4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "##########  EXPERIMENTING WITH EXCEL ###########################################\n",
        "\n",
        "1+1\n",
        "file_path_1 = \"/content/drive/MyDrive/sodata/FB_Test_2020_D_F_I.xlsx\"\n",
        "\n",
        "### file_path_1 = \"/content/drive/MyDrive/sodata/Quant_Fragebogen_20230213_CTL.xlsx\"\n",
        "\n",
        "### file_path_1 = \"/content/drive/MyDrive/sodata/Quant_Regionaler_KS.xlsx\"\n",
        "\n",
        "\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df_1 = pd.read_excel(file_path_1, engine='openpyxl')\n",
        "\n",
        "df_1.iloc[40:100,0:12]\n",
        "\n",
        "df_class = df_1.iloc[:,0]\n",
        "df_text = df_1.iloc[:,9]\n",
        "\n",
        "## extract columns and create new dataframe with relevant columns only\n",
        "\n",
        "df_class_text = pd.concat([df_1.iloc[:,0],df_1.iloc[:,1],df_1.iloc[:,2]], axis=1 )\n",
        "\n",
        "## copy text string from column of answers to column with questions if type = A\n",
        "\n",
        "df_class_text.iloc[:,1][df_class_text.iloc[:,0] == \"A\"] = df_class_text.iloc[:,2]\n",
        "\n",
        "df_test = df_class_text[40:200]\n",
        "\n",
        "print(df_test)\n",
        "\n",
        "df_test[\"class\"] = df_test.iloc[:,1].apply(classify_paragraph)\n",
        "\n",
        "classify_paragraph(\"wie alt sind sie?\")\n",
        "\n",
        "# for excel files of style linkinternational extract rows with \"Q\" in first column = question texts\n",
        "rslt_df = df_1.loc[df_1.iloc[:,0] == \"Q\"].iloc[:,1]\n",
        "\n",
        "# for excel files of style GIM mit Question label\n",
        "rslt_df = df_1.loc[df_1.iloc[:,4] == \"Question\"].iloc[:,9]\n",
        "\n",
        "# for excel files of style GIM ohne  Question label\n",
        "rslt_df = df_1.loc[df_1.iloc[:,8].isnull()].iloc[:,9]\n",
        "\n",
        "rslt_df = df_1.loc[df_1.iloc[:,8].isnull() & ~df_1.iloc[:,9].isnull()].iloc[:,9]\n",
        "\n",
        "df_1.iloc[:,8].isnull() & ~df_1.iloc[:,9].isnull()\n",
        "\n",
        "\n",
        "\n",
        "len(df_1.iloc[:,8].isnull())\n",
        "len(df_1.iloc[:,9].isnull())\n",
        "\n",
        "type(df_1.iloc[:,9])\n",
        "\n",
        "print(rslt_df)\n",
        "## create strings for document object creation\n",
        "## paragraph = \\n\\n before each row for final string\n",
        "\n",
        "len(rslt_df)\n",
        "\n",
        "rows_as_string = \"\\n\\n\".join(rslt_df.astype(str).values.tolist())\n",
        "print(rows_as_string)\n",
        "## rslt_df = df_1.loc[dataframe['Percentage'] != 95]\n",
        "\n",
        "\n",
        "# construct a document FROM DATAFRAME = EXCEL ROWS\n",
        "# note: id_ is added automatically to document\n",
        "\n",
        "so_document_1 = Document(\n",
        "    text= rows_as_string,\n",
        "    metadata={\"filename\": \"<doc_file_name>\",\n",
        "              'file_path': file_path_1,\n",
        "              'file_type': 'xlsx',\n",
        "              \"category\": \"<category>\"},\n",
        " )\n",
        "\n",
        "\n",
        "## construct a document example\n",
        "so_document_2 = Document(\n",
        "    text=\"Das ist mein Text\",\n",
        "    metadata={\"filename\": \"<doc_file_name>\",\n",
        "              'file_path': '/content/drive/MyDrive/sodata/inlinecode.txt',\n",
        "              'file_type': 'xxxx',\n",
        "              \"category\": \"<category>\"},\n",
        " )\n",
        "\n",
        "text_list = [\"dies ist mein Text\",\"und jetzt der zweite Text\"]\n",
        "so_documents = [Document(text=t) for t in text_list]\n",
        "\n",
        "#########################3\n",
        "#########################\n",
        "######### THIS IS THE SOLUTION\n",
        "so_documents_1 = [so_document_1, so_document_2]\n",
        "\n",
        "print(so_documents_1[0])\n",
        "so_documents.metadata['filename']\n",
        "\n",
        "all_documents = documents + [so_document_1]\n",
        "\n",
        "len(documents)\n",
        "len(all_documents)\n",
        "all_documents[3]\n",
        "len([so_document_1])\n",
        "type(so_document_1)\n",
        "type([so_document_1])\n",
        "\n",
        "# append does not work as follows:\n",
        "### do NOT use newlist = list.append(\"x\")\n",
        "\n",
        "documents.append(so_document_1)\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "TyL4guKgl5F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Old embedding loops**"
      ],
      "metadata": {
        "id": "aCpJOl3Ee1rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "###### EMBEDDING THE INPUT DOCUMENTS OLD APPROACH WITH LLAMINDEX DOCUMENTS #####\n",
        "\n",
        "### TESTING EXCELS = SO_DOCUMENTS\n",
        "\n",
        "documents = so_documents\n",
        "\n",
        "# Store  document information\n",
        "doc_titles = []\n",
        "doc_names = []\n",
        "doc_links = []\n",
        "doc_1_ids = []\n",
        "\n",
        "# Store paragraph information\n",
        "para_texts = []\n",
        "vectors = []\n",
        "doc_ids = []\n",
        "\n",
        "\n",
        "for doc in documents:\n",
        "    # Get the document information\n",
        "    file_name = doc.metadata[\"file_name\"]\n",
        "    doc_title = doc.text.split(\"\\n\")[0].strip()\n",
        "    doc_id = doc.id_\n",
        "    doc_titles.append(doc_title)\n",
        "    doc_names.append(file_name)\n",
        "    doc_link = os.path.join(folder_path, file_name)\n",
        "    doc_links.append(doc_link)\n",
        "    doc_1_ids.append(doc_id)\n",
        "        # Get the vector embedding of the document BY PARAGRAPH\n",
        "    paragraphs = split_text_into_paragraphs(doc.text)\n",
        "    for par in paragraphs:\n",
        "      vector = embed_model.get_text_embedding(par)\n",
        "      print(vector[0:10])\n",
        "      vectors.append(vector)\n",
        "      para_texts.append(par)\n",
        "      doc_ids.append(doc_id)\n",
        "\n",
        "\n",
        "len(doc_ids)\n",
        "print(vectors[1][0])\n",
        "len(vectors)\n",
        "len(vectors[0])\n",
        "type(vectors[0])\n",
        "len(para_texts)\n",
        "\n",
        "len(doc_titles)\n",
        "len(doc_1_ids)\n",
        "\n",
        "\n",
        "# Create a DataFrame for the documents information (metadata)\n",
        "df_documents = pd.DataFrame(\n",
        "    {\n",
        "        \"Document Title\": doc_titles,\n",
        "        \"Document Name\": doc_names,\n",
        "        \"Document Link\": doc_links,\n",
        "        \"Document ID\": doc_1_ids,\n",
        "    }\n",
        " )\n",
        "\n",
        "\n",
        "# Create a DataFrame for the paragraphs embeddings\n",
        "df_paragraphs = pd.DataFrame(\n",
        "    {\n",
        "        \"Document ID\": doc_ids,\n",
        "        \"Text\": para_texts,\n",
        "        \"Vector\": vectors,\n",
        "    }\n",
        " )\n"
      ],
      "metadata": {
        "id": "lnB4ggB8e5iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Obsolete Code**"
      ],
      "metadata": {
        "id": "r-wQwYZzRjAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### creating a llamaindex document\n",
        "            so_doc = Document(\n",
        "              text= \"\\n\\n\".join(df.astype(str).values.tolist()),\n",
        "              metadata={\"file_name\": file_name,\n",
        "              'file_path': file_path,\n",
        "              'file_type': 'xlsx',\n",
        "              \"category\": \"<category>\"},\n",
        "               )\n",
        "            so_documents.append(so_doc)\n",
        "\n",
        "\n",
        "1+1"
      ],
      "metadata": {
        "id": "vzxEs56VRnDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######  LOOP THROUGH ALL XLS FILES AND CREATE DOCUMENT FOR EACH XLS FILE\n",
        "#######  OLD VERSION EXTRACTING QUESTION IN DATAFRAME FOR ALL XLS\n",
        "## create initial dataframe with 1 row and 1 column\n",
        "## df = pd.DataFrame({'Text1': [1]})\n",
        "\n",
        "so_documents = []\n",
        "folder_path = \"/content/drive/MyDrive/sodata/\"\n",
        "for file_name in os.listdir(folder_path):\n",
        "        # Check if the file is an Excel file (by extension)\n",
        "        if file_name.endswith('.xlsx') or file_name.endswith('.xls'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            # Read  the Excel file into a DataFrame and extract rows as string\n",
        "            df = pd.read_excel(file_path, engine='openpyxl')\n",
        "            df1 = df.loc[df.iloc[:,0] == \"Q\"].iloc[:,1]\n",
        "            df2 = df.loc[df.iloc[:,4] == \"Question\"].iloc[:,9]\n",
        "            df3 = df.loc[df.iloc[:,8].isnull() & ~df.iloc[:,9].isnull()].iloc[:,9]\n",
        "            df_t = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "            so_doc = Document(\n",
        "              text= \"\\n\\n\".join(df_t.astype(str).values.tolist()),\n",
        "              metadata={\"file_name\": file_name,\n",
        "              'file_path': file_path,\n",
        "              'file_type': 'xlsx',\n",
        "              \"category\": \"<category>\"},\n",
        "               )\n",
        "            so_documents.append(so_doc)\n",
        "\n",
        "so_documents[0]"
      ],
      "metadata": {
        "id": "kke2n2VtTKoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files from local folder\n",
        "folder_path = \"/content/drive/MyDrive/quantdata\"\n",
        "input_files = [\n",
        "    os.path.join(folder_path, filename)\n",
        "    for filename in os.listdir(folder_path)\n",
        "    if filename.endswith(\".docx\")\n",
        " ]\n",
        "\n"
      ],
      "metadata": {
        "id": "a0oonr2NnHdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimented filtering paragraphs = cells with only numbers\n",
        "only_numbers = []\n",
        "for s in paragraphs[200:300]:\n",
        "    only_numbers.append(contains_only_integer(s))\n",
        "\n",
        "filtered_list = [i for (i, v) in zip(paragraphs[200:300], only_numbers) if not v]\n"
      ],
      "metadata": {
        "id": "bJZ0SYN13pSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}